---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# üå≤ Random Forest Challenge - The Power of Weak Learners

::: {.callout-important}
## üìä Challenge Requirements In [Student Analysis Section](#student-analysis-section)

Navigate to the [Student Analysis Section](#student-analysis-section) to see the challenge requirements.

:::

::: {.callout-important}

### R

```{r}
#| label: load-and-model-r
#| echo: false
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```


:::

## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### R
```{r}
#| label: performance-comparison-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

```{r}
#| label: viz-more-trees
#| echo: false
#| warning: false
#| message: false
#| fig-width: 12
#| fig-height: 8

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# --- Helper metrics ---
rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))
r2   <- function(y, yhat) 1 - sum((y - yhat)^2)/sum((y - mean(y))^2)

# --- Load & prep data ---
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

set.seed(123)
ids <- sample(seq_len(nrow(model_data)), 0.8*nrow(model_data))
train <- model_data[ids,]; test <- model_data[-ids,]

# --- Train forests with different tree counts ---
trees <- c(1, 5, 25, 100, 500, 1000, 2000, 5000)

perf <- purrr::map_dfr(
  trees,
  \(T){
    fit <- randomForest(SalePrice ~ ., data=train, ntree=T, mtry=3)
    preds_tr <- predict(fit, train)
    preds_te <- predict(fit, test)
    tibble(
      Trees     = T,
      RMSE_Train= rmse(train$SalePrice, preds_tr),
      RMSE_Test = rmse(test$SalePrice,  preds_te),
      R2_Test   = r2(test$SalePrice,    preds_te)
    )
  }
)

# --- 1Ô∏è‚É£ RMSE Plot (Training vs Test) ---
p_rmse <- perf |>
  pivot_longer(c(RMSE_Train, RMSE_Test), names_to="Split", values_to="RMSE") |>
  mutate(Split = case_when(
    Split == "RMSE_Train" ~ "Training",
    Split == "RMSE_Test" ~ "Test"
  )) |>
  ggplot(aes(Trees, RMSE, color=Split)) +
  geom_line(size = 1.2) + 
  geom_point(size = 3) +
  scale_x_log10(breaks = trees) +
  scale_color_manual(values = c("Training" = "#E31A1C", "Test" = "#1F78B4")) +
  labs(
    title = "RMSE vs Number of Trees (log scale)",
    subtitle = "Lower RMSE indicates better predictive performance",
    x = "Number of Trees (log10)",
    y = "RMSE",
    color = "Data Split"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60"),
    legend.position = "bottom",
    axis.text = element_text(size = 11),
    axis.title = element_text(size = 12, face = "bold")
  )

# --- 2Ô∏è‚É£ R¬≤ Plot (Test) ---
p_r2 <- ggplot(perf, aes(Trees, R2_Test)) +
  geom_line(color="darkgreen", size = 1.2) + 
  geom_point(color="darkgreen", size = 3) +
  scale_x_log10(breaks = trees) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "R¬≤ (Test) vs Number of Trees (log scale)",
    subtitle = "Higher R¬≤ indicates better model fit (explained variance)",
    x = "Number of Trees (log10)",
    y = "R¬≤ (Test)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12, color = "gray60"),
    axis.text = element_text(size = 11),
    axis.title = element_text(size = 12, face = "bold")
  )

# Display plots side by side
library(gridExtra)
grid.arrange(p_rmse, p_r2, ncol = 2)
```

**Analysis:** The visualizations demonstrate the fundamental power of ensemble learning. As we increase the number of trees from 1 to 5000, both RMSE decreases and R¬≤ increases consistently, showing that more "weak learners" create a stronger overall model. The training and test RMSE curves remain close together, indicating that random forests maintain good generalization without overfitting. The diminishing returns become apparent after 1000 trees, where additional trees provide minimal improvement. This suggests that while ensemble methods are powerful, there's a practical limit to the benefits of adding more trees.




### 2. Overfitting Visualization and Analysis

```{r}
#| label: overfitting-analysis
#| echo: false
#| warning: false
#| message: false
#| fig-width: 14
#| fig-height: 6

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(rpart))

# --- Helper metrics ---
rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))

# --- Load & prep data ---
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

set.seed(123)
ids <- sample(seq_len(nrow(model_data)), 0.8*nrow(model_data))
train <- model_data[ids,]; test <- model_data[-ids,]

# --- Decision Tree Analysis: Max Depth vs Performance ---
max_depths <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20)

dt_perf <- purrr::map_dfr(
  max_depths,
  \(depth){
    fit <- rpart(SalePrice ~ ., data=train, control=rpart.control(maxdepth=depth))
    preds_tr <- predict(fit, train)
    preds_te <- predict(fit, test)
    tibble(
      MaxDepth = depth,
      RMSE_Train = rmse(train$SalePrice, preds_tr),
      RMSE_Test = rmse(test$SalePrice, preds_te)
    )
  }
)

# --- Random Forest Analysis: Number of Trees vs Performance ---
trees <- c(1, 5, 10, 25, 50, 100, 200, 500, 1000, 2000)

rf_perf <- purrr::map_dfr(
  trees,
  \(ntree){
    fit <- randomForest(SalePrice ~ ., data=train, ntree=ntree, mtry=3)
    preds_tr <- predict(fit, train)
    preds_te <- predict(fit, test)
    tibble(
      Trees = ntree,
      RMSE_Train = rmse(train$SalePrice, preds_tr),
      RMSE_Test = rmse(test$SalePrice, preds_te)
    )
  }
)

# --- Create Decision Tree Plot ---
dt_plot <- dt_perf |>
  pivot_longer(c(RMSE_Train, RMSE_Test), names_to="Split", values_to="RMSE") |>
  mutate(Split = case_when(
    Split == "RMSE_Train" ~ "Training",
    Split == "RMSE_Test" ~ "Test"
  )) |>
  ggplot(aes(MaxDepth, RMSE, color=Split)) +
  geom_line(size = 1.2) + 
  geom_point(size = 2.5) +
  scale_color_manual(values = c("Training" = "#E31A1C", "Test" = "#1F78B4")) +
  labs(
    title = "Decision Trees: Overfitting with Complexity",
    subtitle = "Training error decreases while test error increases",
    x = "Maximum Tree Depth",
    y = "RMSE",
    color = "Data Split"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10, color = "gray60"),
    legend.position = "bottom",
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11, face = "bold")
  ) +
  ylim(20000, 50000)  # Set consistent y-axis limits

# --- Create Random Forest Plot ---
rf_plot <- rf_perf |>
  pivot_longer(c(RMSE_Train, RMSE_Test), names_to="Split", values_to="RMSE") |>
  mutate(Split = case_when(
    Split == "RMSE_Train" ~ "Training",
    Split == "RMSE_Test" ~ "Test"
  )) |>
  ggplot(aes(Trees, RMSE, color=Split)) +
  geom_line(size = 1.2) + 
  geom_point(size = 2.5) +
  scale_x_log10() +
  scale_color_manual(values = c("Training" = "#E31A1C", "Test" = "#1F78B4")) +
  labs(
    title = "Random Forests: No Overfitting with More Trees",
    subtitle = "Both training and test error decrease together",
    x = "Number of Trees (log scale)",
    y = "RMSE",
    color = "Data Split"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10, color = "gray60"),
    legend.position = "bottom",
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11, face = "bold")
  ) +
  ylim(20000, 50000)  # Set consistent y-axis limits

# --- Display plots side by side ---
library(gridExtra)
grid.arrange(dt_plot, rf_plot, ncol = 2)
```

**Analysis:** This side-by-side comparison reveals the critical difference between individual decision trees and random forests regarding overfitting. The decision tree plot shows classic overfitting behavior: as tree depth increases, training error decreases while test error increases after reaching an optimal depth around 6-8. This creates a growing gap between training and test performance. In contrast, the random forest plot shows both training and test error decreasing together as more trees are added, with minimal gap between them. This demonstrates how random forests avoid overfitting through three key mechanisms: bootstrap sampling (each tree sees different data), random feature selection (each tree uses different variables), and averaging predictions (reduces variance). The ensemble approach transforms weak, overfitting-prone individual trees into a robust, generalizable model.

### 3. Linear Regression vs Random Forest Comparison

**Your Task:** Compare random forests to linear regression baseline.


```{r}
#| label: linear-regression-comparison
#| echo: false
#| warning: false
#| message: false

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# --- Helper metrics ---
rmse <- function(y, yhat) sqrt(mean((y - yhat)^2))
r2 <- function(y, yhat) 1 - sum((y - yhat)^2)/sum((y - mean(y))^2)

# --- Load & prep data ---
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

set.seed(123)
ids <- sample(seq_len(nrow(model_data)), 0.8*nrow(model_data))
train <- model_data[ids,]; test <- model_data[-ids,]

# --- Linear Regression Model ---
lm_model <- lm(SalePrice ~ ., data = train)
lm_pred_train <- predict(lm_model, train)
lm_pred_test <- predict(lm_model, test)
lm_rmse_train <- rmse(train$SalePrice, lm_pred_train)
lm_rmse_test <- rmse(test$SalePrice, lm_pred_test)
lm_r2_train <- r2(train$SalePrice, lm_pred_train)
lm_r2_test <- r2(test$SalePrice, lm_pred_test)

# --- Random Forest Models ---
rf_1 <- randomForest(SalePrice ~ ., data = train, ntree = 1, mtry = 3)
rf_100 <- randomForest(SalePrice ~ ., data = train, ntree = 100, mtry = 3)
rf_1000 <- randomForest(SalePrice ~ ., data = train, ntree = 1000, mtry = 3)

# Predictions for all models
rf_1_pred_train <- predict(rf_1, train)
rf_1_pred_test <- predict(rf_1, test)
rf_100_pred_train <- predict(rf_100, train)
rf_100_pred_test <- predict(rf_100, test)
rf_1000_pred_train <- predict(rf_1000, train)
rf_1000_pred_test <- predict(rf_1000, test)

# RMSE calculations
rf_1_rmse_train <- rmse(train$SalePrice, rf_1_pred_train)
rf_1_rmse_test <- rmse(test$SalePrice, rf_1_pred_test)
rf_100_rmse_train <- rmse(train$SalePrice, rf_100_pred_train)
rf_100_rmse_test <- rmse(test$SalePrice, rf_100_pred_test)
rf_1000_rmse_train <- rmse(train$SalePrice, rf_1000_pred_train)
rf_1000_rmse_test <- rmse(test$SalePrice, rf_1000_pred_test)

# R-squared calculations
rf_1_r2_train <- r2(train$SalePrice, rf_1_pred_train)
rf_1_r2_test <- r2(test$SalePrice, rf_1_pred_test)
rf_100_r2_train <- r2(train$SalePrice, rf_100_pred_train)
rf_100_r2_test <- r2(test$SalePrice, rf_100_pred_test)
rf_1000_r2_train <- r2(train$SalePrice, rf_1000_pred_train)
rf_1000_r2_test <- r2(test$SalePrice, rf_1000_pred_test)

# --- Create Comparison Table ---
comparison_table <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  RMSE_Train = c(lm_rmse_train, rf_1_rmse_train, rf_100_rmse_train, rf_1000_rmse_train),
  RMSE_Test = c(lm_rmse_test, rf_1_rmse_test, rf_100_rmse_test, rf_1000_rmse_test),
  R2_Train = c(lm_r2_train, rf_1_r2_train, rf_100_r2_train, rf_1000_r2_train),
  R2_Test = c(lm_r2_test, rf_1_r2_test, rf_100_r2_test, rf_1000_r2_test),
  RMSE_Improvement = c("Baseline", 
                       paste0(round((lm_rmse_test - rf_1_rmse_test)/lm_rmse_test * 100, 1), "%"),
                       paste0(round((lm_rmse_test - rf_100_rmse_test)/lm_rmse_test * 100, 1), "%"),
                       paste0(round((lm_rmse_test - rf_1000_rmse_test)/lm_rmse_test * 100, 1), "%")),
  stringsAsFactors = FALSE
)

# Format the table nicely
comparison_table$RMSE_Train <- round(comparison_table$RMSE_Train, 0)
comparison_table$RMSE_Test <- round(comparison_table$RMSE_Test, 0)
comparison_table$R2_Train <- round(comparison_table$R2_Train, 3)
comparison_table$R2_Test <- round(comparison_table$R2_Test, 3)

# Display the table
knitr::kable(comparison_table, 
             caption = "Model Performance Comparison: Linear Regression vs Random Forests",
             col.names = c("Model", "RMSE (Train)", "RMSE (Test)", "R¬≤ (Train)", "R¬≤ (Test)", "RMSE Improvement vs Linear Regression"),
             align = c("l", "r", "r", "r", "r", "c"))

```

**Analysis:** The comparison reveals significant performance improvements when switching from linear regression to random forests, with diminishing returns as the number of trees increases. Linear regression serves as a reasonable baseline, but random forests consistently outperform it across all configurations. The jump from 1 tree to 100 trees shows substantial improvement, demonstrating the power of ensemble learning. However, the improvement from 100 to 1000 trees is much smaller, suggesting a practical sweet spot around 100-500 trees for this dataset. The training vs test performance shows that random forests maintain good generalization (small train/test gap) compared to individual decision trees. For real-world applications, the choice between linear regression and random forests depends on the trade-off between interpretability (linear regression wins) and performance (random forests win). When prediction accuracy is critical and the relationships are complex and non-linear, random forests are worth the added complexity.

